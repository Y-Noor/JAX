{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y-Noor/JAX/blob/main/xgboost/jax_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKOhKszvDC-S",
        "outputId": "b850f0c2-f1ad-4f6f-b371-cb3482ee9271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting equinox\n",
            "  Downloading equinox-0.13.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax) (1.16.3)\n",
            "Collecting jaxtyping>=0.2.20 (from equinox)\n",
            "  Downloading jaxtyping-0.3.4-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from equinox) (4.15.0)\n",
            "Collecting wadler-lindig>=0.1.0 (from equinox)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading equinox-0.13.2-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.4-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, equinox\n",
            "Successfully installed equinox-0.13.2 jaxtyping-0.3.4 wadler-lindig-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install jax jaxlib equinox pandas numpy scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTJXldU6baYT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import vmap, jit\n",
        "from functools import partial\n",
        "import equinox as eqx\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 1: REFINED FEATURE ENGINEERING (Skewness + Outliers)\n",
        "# ============================================================================\n",
        "\n",
        "def engineer_features_v4(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Robust Feature Engineering for Leaderboard Stability:\n",
        "    - Log-transforms highly skewed numeric features.\n",
        "    - Preserves Ordinal rankings.\n",
        "    - One-Hot encodes categories.\n",
        "    \"\"\"\n",
        "    train_len = len(train_df)\n",
        "    y_train = train_df['SalePrice'].copy()\n",
        "\n",
        "    # Combine for consistent processing\n",
        "    df = pd.concat([train_df.drop(['Id', 'SalePrice'], axis=1),\n",
        "                    test_df.drop(['Id'], axis=1)], axis=0).reset_index(drop=True)\n",
        "\n",
        "    # 1. Fill Missing Values\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else \"None\")\n",
        "\n",
        "    num_cols = df.select_dtypes(exclude=['object']).columns\n",
        "    for col in num_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # 2. FIX SKEWNESS: Apply Log-Transform to high-skew numeric features\n",
        "    # This prevents 'LotArea' or 'MiscVal' from dominating the Gain calculations\n",
        "    skewed_feats = df[num_cols].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
        "    high_skew = skewed_feats[abs(skewed_feats) > 0.75].index\n",
        "    for feat in high_skew:\n",
        "        df[feat] = np.log1p(df[feat])\n",
        "\n",
        "    # 3. Manual Ordinal Mapping (Preserving Quality Ranks)\n",
        "    qual_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
        "    qual_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC',\n",
        "                 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
        "    for col in qual_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].map(qual_map).astype(int)\n",
        "\n",
        "    # 4. Create Engineered Features\n",
        "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
        "    df['OverallScore'] = df['OverallQual'] * df['OverallCond']\n",
        "\n",
        "    # 5. One-Hot Encoding\n",
        "    df = pd.get_dummies(df)\n",
        "\n",
        "    X_train = df.iloc[:train_len].copy()\n",
        "    X_test = df.iloc[train_len:].copy()\n",
        "\n",
        "    return X_train, X_test, y_train\n"
      ],
      "metadata": {
        "id": "8Njnf9NpiTvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 2: THE JAX-XGBOOST ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "class JAX_XGB_Tree(eqx.Module):\n",
        "    split_features: jnp.ndarray\n",
        "    split_thresholds: jnp.ndarray\n",
        "    leaf_values: jnp.ndarray\n",
        "    max_depth: int\n",
        "\n",
        "    def predict(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        @jit\n",
        "        def walk_tree(sample):\n",
        "            node_idx = 0\n",
        "            for _ in range(self.max_depth):\n",
        "                f = self.split_features[node_idx]\n",
        "                t = self.split_thresholds[node_idx]\n",
        "                go_right = sample[f] > t\n",
        "                node_idx = 2 * node_idx + 1 + go_right.astype(jnp.int32)\n",
        "            return self.leaf_values[node_idx - (2**self.max_depth - 1)]\n",
        "        return vmap(walk_tree)(x)\n",
        "\n",
        "class JAX_XGB_Model:\n",
        "    def __init__(self, n_estimators=450, max_depth=3, learning_rate=0.025, lambda_=25.0, n_bins=64):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_ = lambda_\n",
        "        self.n_bins = n_bins\n",
        "        self.trees = []\n",
        "        self.base_score = None\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def _find_best_split(self, X, G, H, bins, mask):\n",
        "        def get_gain(feat_idx, threshold):\n",
        "            m_l = (X[:, feat_idx] <= threshold) & mask\n",
        "            G_l, H_l = jnp.sum(G * m_l), jnp.sum(H * m_l)\n",
        "            G_r, H_r = jnp.sum(G * mask) - G_l, jnp.sum(H * mask) - H_l\n",
        "            def term(g, h): return (g**2) / (h + self.lambda_)\n",
        "            gain = 0.5 * (term(G_l, H_l) + term(G_r, H_r) - term(jnp.sum(G*mask), jnp.sum(H*mask)))\n",
        "            return jnp.where(jnp.sum(mask) > 0, gain, -1.0)\n",
        "\n",
        "        v_bins = vmap(get_gain, in_axes=(None, 0))\n",
        "        v_feats = vmap(v_bins, in_axes=(0, 0))\n",
        "        gains = v_feats(jnp.arange(X.shape[1]), bins)\n",
        "        best_idx = jnp.argmax(gains)\n",
        "        f_idx, b_idx = jnp.unravel_index(best_idx, gains.shape)\n",
        "        return f_idx, bins[f_idx, b_idx]\n",
        "\n",
        "    def fit(self, X_np, y_np):\n",
        "        X, y = jnp.array(X_np), jnp.array(y_np)\n",
        "        n_samples, n_feats = X.shape\n",
        "        self.bins = jnp.stack([jnp.quantile(X[:, i], jnp.linspace(0, 1, self.n_bins)) for i in range(n_feats)])\n",
        "\n",
        "        self.base_score = jnp.mean(y)\n",
        "        preds = jnp.full_like(y, self.base_score)\n",
        "\n",
        "        print(f\"Training JAX-XGBoost (Strong Regularization: lambda={self.lambda_})\")\n",
        "        for i in range(self.n_estimators):\n",
        "            G, H = preds - y, jnp.ones_like(y)\n",
        "            s_f, s_t = np.zeros(2**self.max_depth - 1, dtype=int), np.zeros(2**self.max_depth - 1)\n",
        "            l_v = np.zeros(2**self.max_depth)\n",
        "            masks = [jnp.ones(n_samples, dtype=bool)]\n",
        "\n",
        "            curr = 0\n",
        "            for d in range(self.max_depth):\n",
        "                nxt = []\n",
        "                for m in masks:\n",
        "                    f, t = self._find_best_split(X, G, H, self.bins, m)\n",
        "                    s_f[curr], s_t[curr] = int(f), float(t)\n",
        "                    nxt.append(m & (X[:, int(f)] <= float(t))); nxt.append(m & (X[:, int(f)] > float(t)))\n",
        "                    curr += 1\n",
        "                masks = nxt\n",
        "\n",
        "            for j, m in enumerate(masks):\n",
        "                l_v[j] = -jnp.sum(G[m]) / (jnp.sum(H[m]) + self.lambda_) if jnp.sum(m) > 0 else 0.0\n",
        "\n",
        "            tree = JAX_XGB_Tree(jnp.array(s_f), jnp.array(s_t), jnp.array(l_v), self.max_depth)\n",
        "            preds += self.learning_rate * tree.predict(X)\n",
        "            self.trees.append(tree)\n",
        "            if (i+1) % 150 == 0:\n",
        "                print(f\"  Round {i+1} RMSE: {jnp.sqrt(jnp.mean((preds-y)**2)):.5f}\")\n",
        "\n",
        "    def predict(self, X_np):\n",
        "        X = jnp.array(X_np)\n",
        "        p = jnp.full(X.shape[0], self.base_score)\n",
        "        for t in self.trees: p += self.learning_rate * t.predict(X)\n",
        "        return np.array(p)\n"
      ],
      "metadata": {
        "id": "j8ywUCEciZG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 3: MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # 1. Load Data\n",
        "    train = pd.read_csv('train.csv')\n",
        "    test = pd.read_csv('test.csv')\n",
        "    test_ids = test['Id']\n",
        "\n",
        "    # AMES OUTLIER REMOVAL (Removes noisy high-leverage points)\n",
        "    train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
        "\n",
        "    # 2. Engineer Features\n",
        "    X_train, X_test, y_train = engineer_features_v4(train, test)\n",
        "    y_log = np.log1p(y_train.values)\n",
        "\n",
        "    # Standardize Features (Ensures JAX binary search is numerically stable)\n",
        "    mu, std = X_train.mean(), X_train.std() + 1e-7\n",
        "    X_train_s, X_test_s = (X_train - mu) / std, (X_test - mu) / std\n",
        "\n",
        "    # 3. Cross Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(y_log))\n",
        "    test_preds_total = np.zeros(len(X_test))\n",
        "\n",
        "    print(f\"Training on {X_train_s.shape[1]} features...\")\n",
        "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_s)):\n",
        "        X_tr, X_val = X_train_s.iloc[tr_idx].values, X_train_s.iloc[val_idx].values\n",
        "        y_tr, y_val = y_log[tr_idx], y_log[val_idx]\n",
        "\n",
        "        # PARAMETER TUNING FOR LEADERBOARD:\n",
        "        # High lambda (25.0) and small learning rate (0.025)\n",
        "        model = JAX_XGB_Model(n_estimators=500, max_depth=3, learning_rate=0.025, lambda_=25.0)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        f_preds = model.predict(X_val)\n",
        "        oof_preds[val_idx] = f_preds\n",
        "        test_preds_total += model.predict(X_test_s.values) / 5\n",
        "        print(f\"Fold {fold+1} Val RMSE: {np.sqrt(mean_squared_error(y_val, f_preds)):.5f}\")\n",
        "\n",
        "    # 4. Final Verification\n",
        "    total_rmse = np.sqrt(mean_squared_error(y_log, oof_preds))\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"ROBUST OOF RMSE: {total_rmse:.5f}\")\n",
        "    print(f\"R2 SCORE:        {r2_score(y_log, oof_preds):.5f}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # 5. Save Final Submission\n",
        "    pd.DataFrame({'Id': test_ids, 'SalePrice': np.expm1(test_preds_total)}).to_csv('submission_jax_robust.csv', index=False)\n",
        "    print(\"Final file saved: submission_jax_robust.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CbDbdfw-idIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7065ba-2fc4-4f99-cced-9794ed25dba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 254 features...\n",
            "Training JAX-XGBoost (Strong Regularization: lambda=25.0)\n",
            "  Round 150 RMSE: 0.12788\n",
            "  Round 300 RMSE: 0.10156\n",
            "  Round 450 RMSE: 0.09282\n",
            "Fold 1 Val RMSE: 0.12426\n",
            "Training JAX-XGBoost (Strong Regularization: lambda=25.0)\n",
            "  Round 150 RMSE: 0.12874\n",
            "  Round 300 RMSE: 0.10302\n",
            "  Round 450 RMSE: 0.09400\n",
            "Fold 2 Val RMSE: 0.12456\n",
            "Training JAX-XGBoost (Strong Regularization: lambda=25.0)\n",
            "  Round 150 RMSE: 0.12643\n",
            "  Round 300 RMSE: 0.10024\n",
            "  Round 450 RMSE: 0.09114\n",
            "Fold 3 Val RMSE: 0.13432\n",
            "Training JAX-XGBoost (Strong Regularization: lambda=25.0)\n",
            "  Round 150 RMSE: 0.12585\n",
            "  Round 300 RMSE: 0.09982\n",
            "  Round 450 RMSE: 0.09170\n",
            "Fold 4 Val RMSE: 0.13641\n",
            "Training JAX-XGBoost (Strong Regularization: lambda=25.0)\n",
            "  Round 150 RMSE: 0.13111\n",
            "  Round 300 RMSE: 0.10579\n",
            "  Round 450 RMSE: 0.09605\n",
            "Fold 5 Val RMSE: 0.10610\n",
            "\n",
            "========================================\n",
            "ROBUST OOF RMSE: 0.12559\n",
            "R2 SCORE:        0.90120\n",
            "========================================\n",
            "Final file saved: submission_jax_robust.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "TZx6da5EKMKt",
        "outputId": "00f8ce2b-9f7a-4f8f-dc2c-c1b5cd9e4ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and Engineering...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'engineer_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-434489519.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining Finished! You can now run the evaluation cell.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmain_with_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-434489519.py\u001b[0m in \u001b[0;36mmain_with_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengineer_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'engineer_features' is not defined"
          ]
        }
      ],
      "source": [
        "# Create empty arrays to store the \"Out-Of-Fold\" results\n",
        "oof_preds = None\n",
        "y_actual_log = None\n",
        "\n",
        "def main_with_evaluation():\n",
        "    global oof_preds, y_actual_log # Make these accessible outside the function\n",
        "\n",
        "    print(\"Loading and Engineering...\")\n",
        "    train = pd.read_csv('train.csv')\n",
        "    test = pd.read_csv('test.csv')\n",
        "    X_train, X_test, y_train = engineer_features(train, test)\n",
        "    y_log = np.log1p(y_train.values)\n",
        "\n",
        "    mu, std = X_train.mean(), X_train.std() + 1e-7\n",
        "    X_train_s = (X_train - mu) / std\n",
        "\n",
        "    # Initialize OOF array\n",
        "    oof_preds = np.zeros(len(y_log))\n",
        "    y_actual_log = y_log\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_s)):\n",
        "        X_tr, X_val = X_train_s.iloc[tr_idx].values, X_train_s.iloc[val_idx].values\n",
        "        y_tr, y_val = y_log[tr_idx], y_log[val_idx]\n",
        "\n",
        "        # Train model\n",
        "        model = JAX_XGB_Model(n_estimators=300, learning_rate=0.05, lambda_=5.0, n_bins=64)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        # Save validation predictions to the OOF array\n",
        "        fold_preds = model.predict(X_val)\n",
        "        oof_preds[val_idx] = fold_preds\n",
        "\n",
        "        print(f\"Fold {fold+1} complete.\")\n",
        "\n",
        "    print(\"\\nTraining Finished! You can now run the evaluation cell.\")\n",
        "\n",
        "main_with_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q9kjS58Kkfw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "def evaluate_model(y_true_log, y_pred_log):\n",
        "    # Convert back to actual Dollars\n",
        "    y_true = np.expm1(y_true_log)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "\n",
        "    # Calculate Metrics\n",
        "    log_rmse = np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true_log, y_pred_log)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    print(\"=\"*40)\n",
        "    print(\"      FINAL MODEL PERFORMANCE\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Kaggle Score (Log RMSE): {log_rmse:.5f}\")\n",
        "    print(f\"R-Squared (Variance):    {r2:.5f}\")\n",
        "    print(f\"Avg. Price Error (MAE):  ${mae:,.2f}\")\n",
        "    print(f\"Avg. Percentage Error:   {mape:.2f}%\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Subplot 1: Actual vs Predicted\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.regplot(x=y_true, y=y_pred, scatter_kws={'alpha':0.3}, line_kws={'color':'red'})\n",
        "    plt.title('Actual vs Predicted Price')\n",
        "    plt.xlabel('Actual Price ($)')\n",
        "    plt.ylabel('Predicted Price ($)')\n",
        "\n",
        "    # Subplot 2: Residuals\n",
        "    plt.subplot(1, 2, 2)\n",
        "    residuals = y_true - y_pred\n",
        "    sns.histplot(residuals, kde=True, color='purple')\n",
        "    plt.title('Distribution of Price Errors')\n",
        "    plt.xlabel('Error ($)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the evaluation using the Out-Of-Fold data\n",
        "evaluate_model(y_actual_log, oof_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8PH_6X9l8_c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Add this to the end of your existing main() function to save the state\n",
        "global fitted_model, train_columns, scaler_mu, scaler_std, qual_map_global\n",
        "\n",
        "fitted_model = model  # The last trained model from the CV loop\n",
        "train_columns = X_train.columns # The list of 256+ columns after OHE\n",
        "scaler_mu = mu\n",
        "scaler_std = std\n",
        "qual_map_global = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
        "\n",
        "# 1. ENTER YOUR HOUSE DETAILS HERE\n",
        "new_house = {\n",
        "    'MSSubClass': 60,\n",
        "    'MSZoning': 'RL',\n",
        "    'LotFrontage': 70.0,\n",
        "    'LotArea': 9000,\n",
        "    'Street': 'Pave',\n",
        "    'LotShape': 'Reg',\n",
        "    'LandContour': 'Lvl',\n",
        "    'Utilities': 'AllPub',\n",
        "    'LotConfig': 'Inside',\n",
        "    'LandSlope': 'Gtl',\n",
        "    'Neighborhood': 'CollgCr',\n",
        "    'Condition1': 'Norm',\n",
        "    'Condition2': 'Norm',\n",
        "    'BldgType': '1Fam',\n",
        "    'HouseStyle': '2Story',\n",
        "    'OverallQual': 7,       # 1-10 Scale\n",
        "    'OverallCond': 5,       # 1-10 Scale\n",
        "    'YearBuilt': 2005,\n",
        "    'YearRemodAdd': 2006,\n",
        "    'RoofStyle': 'Gable',\n",
        "    'RoofMatl': 'CompShg',\n",
        "    'Exterior1st': 'VinylSd',\n",
        "    'Exterior2nd': 'VinylSd',\n",
        "    'MasVnrType': 'None',\n",
        "    'MasVnrArea': 0.0,\n",
        "    'ExterQual': 'Gd',      # Ex, Gd, TA, Fa, Po\n",
        "    'ExterCond': 'TA',\n",
        "    'Foundation': 'PConc',\n",
        "    'BsmtQual': 'Gd',\n",
        "    'BsmtCond': 'TA',\n",
        "    'BsmtExposure': 'No',\n",
        "    'BsmtFinType1': 'GLQ',\n",
        "    'BsmtFinSF1': 700,\n",
        "    'BsmtFinType2': 'Unf',\n",
        "    'BsmtFinSF2': 0,\n",
        "    'BsmtUnfSF': 300,\n",
        "    'TotalBsmtSF': 1000,\n",
        "    'Heating': 'GasA',\n",
        "    'HeatingQC': 'Ex',\n",
        "    'CentralAir': 'Y',\n",
        "    'Electrical': 'SBrkr',\n",
        "    '1stFlrSF': 1000,\n",
        "    '2ndFlrSF': 1000,\n",
        "    'LowQualFinSF': 0,\n",
        "    'GrLivArea': 2000,\n",
        "    'BsmtFullBath': 1,\n",
        "    'BsmtHalfBath': 0,\n",
        "    'FullBath': 2,\n",
        "    'HalfBath': 1,\n",
        "    'BedroomAbvGr': 3,\n",
        "    'KitchenAbvGr': 1,\n",
        "    'KitchenQual': 'Gd',\n",
        "    'TotRmsAbvGrd': 8,\n",
        "    'Functional': 'Typ',\n",
        "    'Fireplaces': 1,\n",
        "    'FireplaceQu': 'Gd',\n",
        "    'GarageType': 'Attchd',\n",
        "    'GarageYrBlt': 2005,\n",
        "    'GarageFinish': 'RFn',\n",
        "    'GarageCars': 2,\n",
        "    'GarageArea': 500,\n",
        "    'GarageQual': 'TA',\n",
        "    'GarageCond': 'TA',\n",
        "    'PavedDrive': 'Y',\n",
        "    'WoodDeckSF': 0,\n",
        "    'OpenPorchSF': 50,\n",
        "    'EnclosedPorch': 0,\n",
        "    '3SsnPorch': 0,\n",
        "    'ScreenPorch': 0,\n",
        "    'PoolArea': 0,\n",
        "    'PoolQC': 'None',\n",
        "    'Fence': 'None',\n",
        "    'MiscFeature': 'None',\n",
        "    'MiscVal': 0,\n",
        "    'MoSold': 5,\n",
        "    'YrSold': 2010,\n",
        "    'SaleType': 'WD',\n",
        "    'SaleCondition': 'Normal'\n",
        "}\n",
        "\n",
        "def predict_single_house(input_dict):\n",
        "    # Convert to DataFrame\n",
        "    df_input = pd.DataFrame([input_dict])\n",
        "\n",
        "    # 2. Ordinal Mapping\n",
        "    qual_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC',\n",
        "                 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
        "    for col in qual_cols:\n",
        "        df_input[col] = df_input[col].map(qual_map_global).fillna(0).astype(int)\n",
        "\n",
        "    # 3. Engineered Features\n",
        "    df_input['TotalSF'] = df_input['TotalBsmtSF'] + df_input['1stFlrSF'] + df_input['2ndFlrSF']\n",
        "    df_input['OverallScore'] = df_input['OverallQual'] * df_input['OverallCond']\n",
        "\n",
        "    # 4. Skewness Log-Transform (must match train logic)\n",
        "    # We apply log1p to numeric columns where the training data was skewed\n",
        "    # Note: For inference, we use the training set's skewness decision\n",
        "    num_cols = df_input.select_dtypes(exclude=['object']).columns\n",
        "    for col in num_cols:\n",
        "        # If your train log-transform threshold was 0.75, we apply it here\n",
        "        # For a single point, we just apply log1p to the known high-skew features\n",
        "        if col in ['LotArea', 'GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'TotalSF']:\n",
        "            df_input[col] = np.log1p(df_input[col])\n",
        "\n",
        "    # 5. One-Hot Encoding & Alignment\n",
        "    df_ohe = pd.get_dummies(df_input)\n",
        "\n",
        "    # ADD MISSING COLUMNS: This is the critical step.\n",
        "    # We ensure the 1 row has all 256+ columns the model expects.\n",
        "    missing_cols = set(train_columns) - set(df_ohe.columns)\n",
        "    for c in missing_cols:\n",
        "        df_ohe[c] = 0\n",
        "\n",
        "    # Reorder columns to match training\n",
        "    df_ohe = df_ohe[train_columns]\n",
        "\n",
        "    # 6. Scaling\n",
        "    df_scaled = (df_ohe - scaler_mu) / scaler_std\n",
        "\n",
        "    # 7. Prediction\n",
        "    log_price = fitted_model.predict(df_scaled.values)\n",
        "    actual_price = np.expm1(log_price)[0]\n",
        "\n",
        "    return actual_price\n",
        "\n",
        "# RUN INFERENCE\n",
        "price = predict_single_house(new_house)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"PREDICTED HOUSE PRICE\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Estimated Value: ${price:,.2f}\")\n",
        "print(\"-\" * 30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPyytY0O3f+w1AOEzTCdlqu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}